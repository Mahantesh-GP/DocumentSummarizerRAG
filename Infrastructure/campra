
Request for Confirmation – Azure Cognitive Search Storage & Tier Sizing for 1.36 Billion Records


---

Email Body:

Hi [Microsoft Support / Azure Architect Team],

We are working on capacity planning for our Azure Cognitive Search production environment and would like to validate our storage and sizing assumptions with Microsoft.

We have approximately 1.36 billion records, and based on our current data structure analysis, we derived the following assumptions:

Average record size (raw SQL data): ~272 bytes (≈ 0.272 KB per record)

Total data volume: ~370 GB (0.272 KB × 1.36 B records)

Azure Search tier considered: S1 (160 GB/partition)

Estimated partitions: ~3 (480 GB total available storage)

Monthly cost per SU (S1): ~$245.28 USD


We understand that Azure Cognitive Search internally creates additional storage structures such as inverted indexes, filter/sort/facet columns, and analyzer token data.
Hence, we’d like to confirm if the raw SQL size (0.272 KB/record) can be used directly for sizing, or whether we should apply an expansion multiplier (2×–5×) to account for indexing overhead.

Additionally, could you please help validate the following for production planning:

1. For our dataset (~1.36 B records ≈ 370 GB raw), would 3 partitions on S1 tier (480 GB) be sufficient after indexing expansion?


2. If not, what would be the approximate equivalent configuration or number of partitions on S2 tier (640 GB per partition)?


3. Is there any official sizing or index expansion guideline Microsoft recommends for accurate estimation?



Our goal is to ensure that our capacity and cost planning are aligned with Microsoft’s best practices for large-scale indexing scenarios.

Thank you in advance for your assistance and clarification.
