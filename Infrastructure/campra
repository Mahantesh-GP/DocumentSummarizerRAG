During our testing, we observed that a single document in Azure AI Search takes about 8.19 KB, even though the raw JSON is much smaller (likely 1–2 KB).
This is expected because Azure Cognitive Search doesn’t just store the document — it also builds several internal data structures to support full-text and phonetic search.

Azure internally creates:

Inverted indexes (word-to-document mappings) for searchable fields

Column stores for filterable, sortable, and facetable fields

Analyzer outputs (phonetic, n-gram, synonyms, etc.) which multiply token count

Metadata and versioning for index maintenance

So the effective storage overhead is typically 2× to 5× the original document size, depending on schema complexity.
Since we are using phonetic analyzers and multiple searchable fields, our expansion ratio is around 5×, which explains the ~8 KB per document size we see.

As a result:

Raw data (~1.36 B docs × 1–2 KB) becomes about 10–11 TB of actual index size after analyzer and inverted-index expansion.

This means roughly 68 partitions are needed on S1 tier (160 GB each).

The observed cost aligns with this (~$16 K/month for 1 replica or ~$33 K with HA).
